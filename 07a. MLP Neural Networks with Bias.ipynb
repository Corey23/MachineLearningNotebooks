{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Neural Networks\n",
    "In this notebook we will build several different multi-layer perceptron models. Each time, we will add some common functionality to the model. We will start with building an easy to analyze system and build the overall complexity each time.  \n",
    "\n",
    "## Getting some data to test with:\n",
    "Let's start by grabbing some data for a (small) hand written digits dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n",
      "-0.5 0.5\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# lets load up the handwritten digit dataset\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "ds = load_digits()\n",
    "X = ds.data/16.0-0.5 # normalize the input, very important\n",
    "y = ds.target\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(np.min(X),np.max(X))\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True,)\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = X[y == i][0].reshape(8, 8)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEYCAYAAABbd527AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAXN0lEQVR4nO3dT2gV1/vH8TM/f0KslCYxNmgKuRjBiEYLRmkV6iKipS5MF239s0oWLRrpIi1EMLowFlxooMUE7CJZ1ZqKaBYV2urChWn5moCYgqY2koCpaG6Mpa0NBLnfxQ/6Q59n0jN3cpM793m/lh8md06fzn06vefMmSCTyTgAgB3/M98DAADMLRo/ABhD4wcAY2j8AGAMjR8AjPnfKAeXlZVlUqlU1ifTVhCNjo6KLM45RkZGXDqdDrL+gByLW8Px8XGRjY2NiWz9+vUiCwK/suRzDePWTzM8PCwy7RwLFizw/syBgYF0JpNZGmdcuRK3hg8ePBDZw4cPRRbnGnSusGv4xx9/iOyXX34R2Zo1a0RWVFTkdY6ZvseRGn8qlXL9/f1R/uQ5U1NTIjtw4IDIurq6sj5HbW1t1n87F+LWsLOzU2Stra0iu379ush8L5h8rmHc+mneffddkXV3d4usuLjY+zODIJB3NHkibg2PHz8usvb2dpHFuQadK+waXr16VWTbtm0T2YULF0S2atUqr3PM9D3mpx4AMIbGDwDG0PgBwJhIv/HH1dvbK7J8/j15PmnzIc7pv6WuXr061mdG+d016bTfVgcHB0VmqSZhhoaG1PzIkSMiq6+vFxk1DP/OHT16VGQlJSUiq6ysnPUxOccdPwCYQ+MHAGNo/ABgDI0fAIzJyeRu2ITGF198IbJjx46J7MmTJ97nivJQTZIcPnxYzR8/fiyya9euiWz58uUi0ybgnIv3wFy+CpuY1B6SOXfunMi0mmgPGxayN998U82rqqpEpj3w1tjYKLK2tjb1MysqKiKOLv/4PqDqnHN9fX0iu3HjhshyNUHOHT8AGEPjBwBjaPwAYAyNHwCMofEDgDE5WdWjbc3gnHO3b98WWV1dnci0bV9LS0vVzyyElRY9PT0i07ZmcE5fgbJkyRKRTU5OiszS9hjaihLn9FUlH3zwgci0fePDtsPVruGk0VZBadeQc841NzeLTNuPX1vpo21L4Jxzp06d+rch5j1tJZhWA+f0Gmrfz08++URk77zzjvqZUa5D7vgBwBgaPwAYQ+MHAGNo/ABgTOzJXe29k7t371aP1SY0NNp+31euXIk2sAS5e/eu97Hathdh2zu8aNOmTd7nSRJtj33tkXjnnHvjjTdEFjYR/KJCmMQNc/78ee9j7927JzLfGmrf7ULxww8/eB+rbXvh+y5jrf7OMbkLAJgBjR8AjKHxA4AxNH4AMCb25O7LL78ssrCn87SJip9++snrPFu2bIk2sAT59NNPRRb21KT2JKB2rDZ5VKhP7mqTWtre5s459/XXX4ssbO9+S1pbW72P9Z2g1a7BQn1/hnP6d1N7V4ZzzjU1NWV9nrDFM1Fwxw8AxtD4AcAYGj8AGEPjBwBjYk/ualvVai8Ed865sbExkdXU1IhMe8I3Vy8dzgfaP1vYNrWfffaZyBYtWiSyXbt2xR9YgoVNZGu59uSv9lJ2a8ImfLUt0rXJyjNnzsz6mPKZNnEd1gu1BQXV1dUia2hoEJm2jXhU3PEDgDE0fgAwhsYPAMbQ+AHAGBo/ABiTk5eth1m8eLHItO0GPvzww7kYTiJpe+9rW2RQQ3+nT58WWUdHxzyMJBnCVqq8aMOGDTkeSXLdvHnT67i2tracnJ87fgAwhsYPAMbQ+AHAGBo/ABgTZDIZ/4ODYNw5N5q74cyKykwms3S+BxGGGsaTkPo5Rw1nAzWMJ7R+kRo/ACD5+KkHAIyh8QOAMTR+ADCGxg8AxtD4AcAYGj8AGEPjBwBjaPwAYAyNHwCMofEDgDE0fgAwhsYPAMZEevViWVlZJpVKZX2yBw8eeB23bNmyrM8xMjLi0ul0kPUH5Fguavjbb7+JbM2aNSIrKiryOkc+1zBu/aanp0U2NDQkMq1+QeBfkoGBgXS+7iwZt4bPnj0T2d27d0VWXV2d9TmcK+wa3r9/X2TpdFpkr7/+etbnmOl7HKnxp1Ip19/fn/VAjh8/7nVca2tr1ueora3N+m/nQi5qeOTIEZFduHBBZKtWrfI6Rz7XMG79xsbGRLZ161aRXb9+XWS+/+F0zrkgCPJ2y964NXzy5InIdu7cKTKthlEUcg0/+eQTkXV3d4sszjlm+h7zUw8AGEPjBwBjIv3U40v732nn9J8k2tracjGERJmamhLZyZMn1WO1Gm7evFlk5eXl8QdWgN5//32R1dTUzMNIkuvs2bMiu3379jyMJP81Njaq+aVLl0Q2ODiY49H8P+74AcAYGj8AGEPjBwBjaPwAYEzsyV3fddHOOdfc3Cwybc2+tk64uLg48tiSYu3atSIbHh5Wj62qqhLZvn37RFbI9fLR09Oj5n19fSK7ceOGyCYmJkRWUVERf2AJEraGvKmpSWRaDa19j7VrTpvEdc65H3/8UWRzeX1xxw8AxtD4AcAYGj8AGEPjBwBjaPwAYEzsVT3t7e0iC1uRsmfPHpFpM+H79+8Xmbb1sHPRdkzMB5lMRmzR8PjxY3FcSUmJ+vfnzp0T2caNG0WmfWacXU/zmbayTLuGnHOuoaHB6zNfe+01kU1OTqrHFupKle3bt6t5fX29yLSVaYsWLRLZnTt31M/03Tk2n2nfTa1WzjlXWVkpMu067u3tFdnevXvVz4xyHXLHDwDG0PgBwBgaPwAYQ+MHAGNiT+5q+8OHTYJpk0Xasdr+8oUiCAIxIf3999+L47QJ25nyF5WWlkYfXEJp7yEOuwY1nZ2dXscNDAyoeV1dnfe58pX23uGwGmqvCAx7f8SLrl69quaFMLl74sQJkR06dEg9dvny5V6fqf070BZuOBdt8QZ3/ABgDI0fAIyh8QOAMTR+ADAm9uSu9rRYV1eXeuyOHTtEtnv3bpF98803IkvaE7pR1NbWiixsYm3nzp0i0/aYD3vJcyHS6nflyhX12NOnT4tM2zNdW2BQXV0dfXAJoU2uau/PcE6vd9jT+i8qhInwMFoNL168qB6rLSjQ3nPQ0dEhsrAnd6Pgjh8AjKHxA4AxNH4AMIbGDwDGxJ7cjaKsrExk2vbD1l5qrQnbYtX3CcdCngz3ETaJuGXLFpFpT1F+/PHHIrN2XZ46dco71+q6b98+kRXCE7qzQZsM17ZwPnDgQE7Ozx0/ABhD4wcAY2j8AGAMjR8AjKHxA4Axc7qqx/cF1v39/SLTHhMHonrxRffORdu7H/5YwRPu3r17ItuwYcOcnZ87fgAwhsYPAMbQ+AHAGBo/ABgTZDIZ/4ODYNw5N5q74cyKykwms3S+BxGGGsaTkPo5Rw1nAzWMJ7R+kRo/ACD5+KkHAIyh8QOAMTR+ADCGxg8AxtD4AcAYGj8AGEPjBwBjaPwAYAyNHwCMofEDgDE0fgAwhsYPAMZEevViWVlZJpVK/etxYRu/jY2NiWzBggUiW7ZsWZRhPWdkZMSl0+kg6w/IMd8ahhkeHhbZ9PS0yKqrq7M+Rz7XMG79xsfHRfbw4UORrV27NutzOOfcwMBAOl93loxbQ+16u3XrlsjWrVsnsoULF3qfp5Br+PTpU5Fpr2OMcx3O9D2O1PhTqZT6PtwXae81dc65w4cPi6ykpERkra2tUYb1nHx/N69vDcO8++67Inv06JHIrl+/nvU58rmGcevX2dkpsvb2dpHFOYdzzgVBkLdb9satoXYDp71P+/LlyyKrqKjwPk8h11D72927d3sd52um7zE/9QCAMTR+ADAm0k89vk6ePKnm3d3dIvvxxx9zMYTEC/tfvEuXLomsubk5x6NJnrD6NTU1iayhoSHXwykoBw8e9Dpu8eLFOR5J/gv72Xv79u0iW716da6H8w/u+AHAGBo/ABhD4wcAY2j8AGBM7MldbfLiyJEj6rE3btwQ2apVq+IOIfGuXr0qsm3btqnHbt68WWSTk5Mi09b7a5PrzjlXXFz8LyPMb9o1qE2eOedcVVWVyLS1/dq/k7q6uixGl1xaXZzTFxhotAfjkn6tRRW20EX7zs4l7vgBwBgaPwAYQ+MHAGNo/ABgDI0fAIyJvapndNR/Az1ti9GhoSGR7dy5U2Tffvut+pmFsCro9OnT3se+8cYbItNW62irBjZs2KB+ZpzdUPNBb2+vyMJWTXz//fcim5iYEJm2qqqjo0P9zAMHDvzbEPNeT0+PyLTtLZzTd9TV6n3o0CGvv3XOua6urn8bYt7TtgkJW+EYVoe5wh0/ABhD4wcAY2j8AGAMjR8AjMnJfvxh9uzZI7LBwUGRae+V/eOPP3Iypnxw4sQJkWl1cU5/TaCmvr5eZO+9916kcSWFNjkb5j//+Y/IfGu6d+9e7/MkTZQa+m43oL0SNMpChqTRrq0wWg37+vpEVlpaKrKw929EWaTBHT8AGEPjBwBjaPwAYAyNHwCMiT25qz05+/fff6vHak9Yant7t7W1iay2tjb64BJCq+Gvv/6qHrty5UqRlZeXi+zixYvxB5YQ2pOzYU90f/TRRyLTFhNoL2Av5L3ktRqGPZGsPeV7+PBhkWnvNCgqKspidMmg1SvsHQ5ffvmlyLT+qE3kbtq0KYvRPY87fgAwhsYPAMbQ+AHAGBo/ABiTkyd3wyZwduzY4fX3hfqEaa68+uqr8z2EvBM2qXbmzBmRaVswt7S0zPqYCkVZWZnItAnyQp7I9RW2yKCqqkpkNTU1IsvVlt/c8QOAMTR+ADCGxg8AxtD4AcAYGj8AGDOn+/FrWzFs3rxZZIXwAvVceeutt0SmvbAeusuXL4tMe/E112C41157TWTaKhWEW7JkichWrFgxZ+fnjh8AjKHxA4AxNH4AMIbGDwDGBJlMxv/gIBh3zo3mbjizojKTySyd70GEoYbxJKR+zlHD2UAN4wmtX6TGDwBIPn7qAQBjaPwAYAyNHwCMofEDgDE0fgAwhsYPAMbQ+AHAGBo/ABhD4wcAY2j8AGAMjR8AjKHxA4AxNH4AMCbSO3fLysoyqVQq65NNT0+L7NatWyJbt26dyBYuXOh1jpGREZdOp4Poo5sbcWs4Pj4usomJCZFVV1dnfY58rmHc+mlGRkZE9uqrr4rspZde8v7MgYGBdL5uKRy3hs+ePRPZ4OCgyLRrsKioyPs8hVzDyclJkY2Oyl2e169fL7Ig8PtqzvQ9jtT4U6mU6+/vj/InzxkbGxOZ9uJm7YXYFRUVXueora2NPrA5FLeGnZ2dIvvqq69Edv369azPkc81jFs/TWNjo8gOHDggsih1CYIgb/dqj1vDJ0+eiEx7UfiFCxdEFuUl9oVcw56eHpHt379fZNr32Pc/njNdr/zUAwDG0PgBwJhIP/XE1d7e7nXckSNHRNbV1TXbw8lrU1NTat7U1DTHIyks2k9l3d3dIvO9Vi06e/as13GVlZU5HklyaT/raL/75wp3/ABgDI0fAIyh8QOAMTR+ADAmJ5O7Q0NDau47YZbP68jnSm9v73wPIdHCrsHW1lavv//rr79EVlxcHGdIiRO2wED7Hm/dulVkUR7WKlTHjx9Xc9+JXO3fwWzUlTt+ADCGxg8AxtD4AcAYGj8AGEPjBwBjYq/q0Xbq27lzp3psfX29yC5duiSyTZs2xRxVsmgz99oj3fB36NAhNT9//rzItm3bluvhJFLYDq/Dw8Miq6mpyfVw8p7WC7XtZ6LI1Uoy7vgBwBgaPwAYQ+MHAGNo/ABgTOzJ3ba2NpFpkz9RbNy4UWQNDQ3qsYWwT782iRb2SHdJSYnXsdqj4gcPHlQ/M+lbEWivwLt27Zp67MWLF0Wm1fTBgwfe5/d9LWjSHD161PtYrd7athnl5eXq3yf9GnRO74Vxae/X7ejoUI/VXhcahjt+ADCGxg8AxtD4AcAYGj8AGBN7cnfPnj0iu3fvnnrso0ePRKZNBGuTF7t27cpidMmlTTg657+Pt/bEYNi/l6RPkP/+++8iC6uTNlmm0V7K3tLSEm1gCaItBujr6/P+e63e1dXVIivkRRpVVVUiC/sea0pLS0XW3NwsstnY2YA7fgAwhsYPAMbQ+AHAGBo/ABgTe3JXezG69nSkc/q2pStWrBCZNpFbqE9HOudcXV2dyB4/fqwe29jYKLLu7m6RZTKZ+ANLCK1+Uf75V65cKbJCmGyMQpt01SYbwzQ1NYns/v37Iivk77H25GzY07Ta91ibII/yNG4U3PEDgDE0fgAwhsYPAMbQ+AHAGBo/ABgTe1VPFA8fPhTZ1q1bRVbIM/9xaY+Aa4+Kw5/2onBtBVoh7BkfRvvOha0oGRsbE1lra6vXZ+L/7Nu3T2TvvfeeyKampkRWVFQU+/zc8QOAMTR+ADCGxg8AxtD4AcCYIMqj7UEQjDvnRnM3nFlRmclkls73IMJQw3gSUj/nqOFsoIbxhNYvUuMHACQfP/UAgDE0fgAwhsYPAMbQ+AHAGBo/ABhD4wcAY2j8AGAMjR8AjKHxA4AxNH4AMIbGDwDG0PgBwJhIr14sKyvLpFKprE/27NkzkQ0ODopMe2Xb0qV+m/SNjIy4dDodRB/d3Ihbw+npaZENDw+LbNWqVSILAr+y5HMN49ZPc/PmTZGVl5eLbNmyZd6fOTAwkM7XnSVzUcM7d+6ITHtFYJTzFnINnz59KrLbt2+LbN26dSJbuHCh1zlm+h5HavypVMr19/dH+ZPnaO8xXbFihchaWlpEFvb+zxfV1tZGHtdciltD7X2n77//vsiuXr0qMt93deZzDePWT1NaWiqy/fv3i0x7r2yYIAjydsveXNRwy5YtItNuPrq6urw/s5BrqP3txo0bRXb58mWR+b7LeKbvMT/1AIAxNH4AMCbSTz1xNTc3i2xyclJkdXV1czGcROrt7RVZX1+fyCYmJkTm+7+Ihayzs1Nk2jX49ttvz8VwEkmroXYNfv7553MxnLw2NTWl5tu3b/f6+z///HM2h/MP7vgBwBgaPwAYQ+MHAGNo/ABgTE4md48fP67m3d3dXn+vrf+1JmxSyHctea4mhZIirH5NTU1zPJLk0p4Zcc7/Gnz55ZdncziJdPjwYTXXFhRoKisrZ3M4/+COHwCMofEDgDE0fgAwhsYPAMbQ+AHAmNirerSZ//b29rgfa17YLobzvRogKXx3cw2zdu1akYWtFPLd9TRptO1Bovjyyy9F9tlnn6nHFkIN4/bCkpISkY2Oyg1KtS3DnXOuuLjY+1zc8QOAMTR+ADCGxg8AxtD4AcCY2JO7Bw8eFJnvBGSYlStXiuzMmTPqsYWwd7/2SsqwrQWqqqpEpr1z9+effxaZNmHpXPIn1rTXTPpuDxJm+fLlItPeJ+FctFcy5quhoSGRRbkGte+8NrEZ1huivJIxX8Vd1KLVprq6WmT19fXq31+8eNH7XNzxA4AxNH4AMIbGDwDG0PgBwJicTO6GGRwcFJk2MVlTUyOyV155JdrAEiRs0lCj1UuzceNG7/OcOnXK+/z5KJ1Oi0x7CtI5/4UHDQ0NIvv000+jDSxBbt686X2s9uTo48ePRVZaWiqytra2SONKEu2p5E2bNqnHavv0a3U9duyYyGajF3LHDwDG0PgBwBgaPwAYQ+MHAGNiT+5qT86GPU3b2dkpsq+++kpkUZ5AKwTaFsK1tbXqsf39/SLTnlK9c+eOyAp1q+YPPvhAZLt27VKP1bYa3r17t8i0ibqkP+E8E62GWhYmCAKRaU+YVlRURBpXkmjXR1gNv/vuO5FpCw9ytTMBd/wAYAyNHwCMofEDgDE0fgAwhsYPAMbEXtUTxd69e0Wm7WGtvdS6kFdUaCt4wlb1aHv3a6t6tMe/C7mGLwr7Z9VW+2jbO2jvMwj7d4LwPeKha2lpEZn2cvpc4Y4fAIyh8QOAMTR+ADCGxg8AxgSZTMb/4CAYd86N5m44s6Iyk8ksne9BhKGG8SSkfs5Rw9lADeMJrV+kxg8ASD5+6gEAY2j8AGAMjR8AjKHxA4AxNH4AMIbGDwDG0PgBwBgaPwAYQ+MHAGP+Cwoc/U8ze5JkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True,)\n",
    "ax = ax.flatten()\n",
    "digit = 4\n",
    "x_digits = X[y == digit]\n",
    "for i in range(25):\n",
    "    img = x_digits[i].reshape(8, 8)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1437, 64)\n",
      "(1437,)\n",
      "(360, 64)\n",
      "(360,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,y,test_size = 0.2)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "# Single Layer Neural Networks are just Linear and Logistic Regression\n",
    "The single layer network can be summarized with the notation given by previously covered topics (linear and logistic regression). When we calculated objective functions for linear regression and logistic regression we used the maximum likelihood priciple, showing that (under Gaussian noise) this led to minimizing the squared error of the output.\n",
    "\n",
    "This is identical to the objective function for generic, single layer neural networks:\n",
    "$$ \\sum_i^M (\\mathbf{y}^{(i)}-\\hat{\\mathbf{y}}^{(i)})^2 $$\n",
    "\n",
    "Here, the $\\mathbf{y}^{(i)}$ is a one-hot-encoded representation of the class for the $i^{th}$ instance.\n",
    "Adding in terms of single layer neural network explicitly:\n",
    "$$ J(\\mathbf{W}) = \\sum_i^M (\\mathbf{y}^{(i)}-\\phi(\\mathbf{W}\\cdot \\mathbf{x}^{(i)}))^2 $$\n",
    "\n",
    "Adaline network objective function:\n",
    "$$ J(\\mathbf{W}) = \\sum_i^M (\\mathbf{y}^{(i)}-\\mathbf{W}\\cdot \\mathbf{x}^{(i)})^2 $$\n",
    "\n",
    "To solve, we need the gradient:\n",
    "$$  \\nabla J(\\mathbf{W}) $$\n",
    "\n",
    "And then we can use the Widrow-Hoff learning Rule:\n",
    "$$ \\mathbf{W}\\leftarrow\\mathbf{W} +\\eta\\nabla J(\\mathbf{W}) $$\n",
    "\n",
    "Which can be separated for each row (they are independent):\n",
    "$$ \\mathbf{w}\\leftarrow\\mathbf{w} +\\eta[ \\mathbf{X}*(\\mathbf{y}-\\mathbf{\\hat{y}})] $$\n",
    "\n",
    "$$ \\mathbf{w}_{row}\\leftarrow\\mathbf{w}_{row} +\\eta[ \\mathbf{X}*(\\mathbf{y}_{row}-\\mathbf{\\hat{y}}_{row})] $$\n",
    "\n",
    "The same is true for the update equation of the perceptron with a sigmoid activation function:\n",
    "$$ \\mathbf{w}\\leftarrow\\mathbf{w} +\\eta[ \\mathbf{X}*(\\mathbf{y}-\\mathbf{g(x)})] $$\n",
    "\n",
    "$$ \\mathbf{w}_{row}\\leftarrow\\mathbf{w}_{row} +\\eta[ \\mathbf{X}*(\\mathbf{y}_{row}-\\mathbf{g(x)}_{row})] $$\n",
    "\n",
    "This means that the update equations are identical to the case of linear and logistic regression, using one-versus-all! So all of this we have already looked at and solved! Yay!! But, what about when we add more layers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Case\n",
    "In the multilayer perceptron, we need to adjust some notation:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/eclarson/MachineLearningNotebooks/master/PDF_Slides/MultiLayerNetwork.png\" width=\"500\">\n",
    "\n",
    "\n",
    "## Formulation of multi-layer perceptron\n",
    "Our objective function becomes:\n",
    "$$ J(\\mathbf{W}) = \\sum_k^M (\\mathbf{y}^{(k)}-\\mathbf{a}^{(L)})^2 $$\n",
    "\n",
    "And the update equation becomes:\n",
    "$$  w_{i,j}^{(l)} \\leftarrow w_{i,j}^{(l)} - \\eta \\frac{\\partial J(\\mathbf{W})}{\\partial w_{i,j}^{(l)}}$$\n",
    "\n",
    "However, this is not tractable to compute by hand for each network architecture, so we expand the gradient calculation using the chain rule to get partial derivatives:\n",
    "$$  \\frac{\\partial J(\\mathbf{W})}{\\partial w_{i,j}^{(l)}} = \\frac{\\partial J(\\mathbf{W})}{\\partial\\mathbf{z}^{(l)}} \\frac{\\partial\\mathbf{z}^{(l)}}{\\partial w_{i,j}^{(l)}}   $$\n",
    "\n",
    "The second term in the partial derivatives is easy to calculate as $a_j^{(l)}$ (see lecture notes), such that:\n",
    "$$  \\frac{\\partial J(\\mathbf{W})}{\\partial w_{i,j}^{(l)}} = \\frac{\\partial J(\\mathbf{W})}{\\partial\\mathbf{z}^{(l)}} a_j^{(l)}   $$\n",
    "\n",
    "And our update equation would be:\n",
    "$$  w_{i,j}^{(l)} \\leftarrow w_{i,j}^{(l)} - \\eta \\frac{\\partial J(\\mathbf{W})}{\\partial\\mathbf{z}^{(l)}} a_j^{(l)}$$\n",
    "\n",
    "To calculate the other term, we need to continue to use the chain rule:\n",
    "$$ \\frac{\\partial J(\\mathbf{W})}{\\partial\\mathbf{z}^{(l)}} = \\left[\\frac{\\partial \\mathbf{z}^{(l+1)} }{\\partial\\mathbf{z}^{(l)}}\\right]^T \\frac{\\partial J(\\mathbf{W})}{\\partial\\mathbf{z}^{(l+1)}}  $$\n",
    "\n",
    "After some derivation, we can see that: \n",
    "$$ \\frac{\\partial \\mathbf{z}^{(l+1)} }{\\partial\\mathbf{z}^{(l)}} = \\text{diag}[\\mathbf{a}^{(l+1)}*(1-\\mathbf{a}^{(l+1)})] \\cdot\\mathbf{W}^{(l+1)}  $$\n",
    "\n",
    "and\n",
    "$$ \\frac{\\partial J(\\mathbf{W})}{\\partial\\mathbf{z}^{(l)}} = \\text{diag}[\\mathbf{a}^{(l+1)}*(1-\\mathbf{a}^{(l+1)})] \\cdot\\mathbf{W}^{(l+1)} \\frac{\\partial J(\\mathbf{W})}{\\partial\\mathbf{z}^{(l+1)}} $$\n",
    "\n",
    "This set up a recurrence relation such that if we know $\\frac{\\partial J(\\mathbf{W})}{\\partial\\mathbf{z}^{(l)}}$ for the final layer, we can use it to get the derivatives for the previous layers. For a two layer network, the final layer gradient can be formulated as:\n",
    "$$  \\frac{\\partial J(\\mathbf{W})}{\\partial\\mathbf{z}^{(2)}} = \\frac{\\partial}{\\partial\\mathbf{z}^{(2)}} (\\mathbf{y}^{(k)}-\\phi(\\mathbf{z}^{(2)}))^2 $$\n",
    "\n",
    "$$  \\frac{\\partial J(\\mathbf{W})}{\\partial\\mathbf{z}^{(2)}}  = -2(\\mathbf{y}^{(k)}-\\mathbf{a}^{(3)})*\\mathbf{a}^{(3)}*(1-\\mathbf{a}^{(3)}) $$\n",
    "\n",
    "where $\\mathbf{y}^{(k)}$ is the $k^{th}$ instance and $\\mathbf{a}^{(3)}$ is the feedforward result for the $k^{th}$ instance. We can make this more explicit by writing the $k^{th}$ instance of $\\mathbf{a}^{(3)}$ as $[\\mathbf{a}^{(3)}]^{(k)}$. Now the update becomes:\n",
    "\n",
    "$$  \\frac{\\partial J(\\mathbf{W})}{\\partial [\\mathbf{z}^{(2)}]^{(k)}}  = -2\\left(\\mathbf{y}^{(k)}-{[\\mathbf{a}^{(3)}]^{(k)}}\\right)*{[\\mathbf{a}^{(3)}]^{(k)}}*\\left(1-{[\\mathbf{a}^{(3)}]^{(k)}}\\right) $$\n",
    "\n",
    "Also recall that $\\cdot$ represent matrix multiplication and $*$ represents elementwise or row-wise multiplication. The update equation can be given by:\n",
    "\n",
    "$$ \\mathbf{W}^{(l)} \\leftarrow \\mathbf{W}^{(l)} -\\eta \\frac{\\partial J(\\mathbf{W}^{(l)})}{\\partial [\\mathbf{z}^{(l)}]^{(k)}} \\cdot {[\\mathbf{a}^{(l)}]^{(k)}} $$\n",
    "\n",
    "For single elements stochastic gradient descent. Or can be given by the following for batch gradient descent:\n",
    "\n",
    "$$ \\mathbf{W}^{(l)} \\leftarrow \\mathbf{W}^{(l)} -\\eta \\sum_k\\frac{\\partial J(\\mathbf{W}^{(l)})}{\\partial {[\\mathbf{z}^{(l)}]^{(k)}}} \\cdot {[\\mathbf{a}^{(l)}]^{(k)}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming a Two Layer Perceptron\n",
    "Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "Original Author: Sebastian Raschka\n",
    "\n",
    "This is the optional book we use in the course, excellent intuitions and straightforward programming examples please note, however, that this code has been heavily manipulated to reflect our assumptions and notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "#Original Author: Sebastian Raschka\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted\n",
    "# it only has internal classes to be used by classes that will subclass it\n",
    "# Start with the following functions:\n",
    "#    init\n",
    "#    encode_labels\n",
    "#    initialize weights\n",
    "#    sigmoid\n",
    "#    add bias (vector of ones)\n",
    "#    objective function (cost and regularizer)\n",
    "class TwoLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_)*self.n_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden, self.n_features_) # reshape to be W\n",
    "        b1 = np.zeros((self.n_hidden, 1))\n",
    "        \n",
    "        W2_num_elems = (self.n_hidden)*self.n_output_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_output_, self.n_hidden)\n",
    "        b2 = np.zeros((self.n_output_, 1))\n",
    "        \n",
    "        return W1, W2, b1, b2\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_) * np.sqrt(np.mean(W1 ** 2) + np.mean(W2 ** 2))\n",
    "    \n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's add in the following functions:\n",
    "#    feedforward\n",
    "#    fit and predict\n",
    "class TwoLayerPerceptron(TwoLayerPerceptronBase):\n",
    "    def _feedforward(self, X, W1, W2, b1, b2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = X.T\n",
    "        Z1 = W1 @ A1 + b1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        Z2 = W2 @ A2 + b2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # need to vectorize this computation!\n",
    "        # See additional code and derivation below!\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2, self.b1, self.b2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\"\"\"\n",
    "        \n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.b1, self.b2 = self._initialize_weights()\n",
    "\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            # feedforward all instances\n",
    "            A1, Z1, A2, Z2, A3 = self._feedforward(X_data,self.W1,self.W2, self.b1, self.b2)\n",
    "            \n",
    "            cost = self._cost(A3,Y_enc,self.W1,self.W2)\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "            # compute gradient via backpropagation\n",
    "            gradW1, gradW2, gradb1, gradb2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, Y_enc=Y_enc,\n",
    "                                              W1=self.W1, W2=self.W2)\n",
    "\n",
    "            self.W1 -= self.eta * gradW1\n",
    "            self.W2 -= self.eta * gradW2\n",
    "            self.b1 -= self.eta * gradb1\n",
    "            self.b2 -= self.eta * gradb2\n",
    "            \n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized coding operations in gradient\n",
    "So we need to derive equations for vectorized operations. These can be derived by specifically writing out our current gradient calculations. If we plug in the equations for the final layer gradient, we have that:\n",
    "\n",
    "$$  \\nabla_k^{(2)} = -2\\left(\\mathbf{y}^{(k)}-{[\\mathbf{a}^{(3)}]^{(k)}}\\right){[\\mathbf{a}^{(3)}]^{(k)}}\\left(1-{[\\mathbf{a}^{(3)}]^{(k)}}\\right)\\cdot {[\\mathbf{a}^{(2)}]^{(k)} } $$\n",
    "\n",
    "where we use $\\nabla_k^{(2)}$ in place of $\\frac{\\partial J(\\mathbf{W}^{(l)})}{\\partial _k\\mathbf{z}^{(2)}} \\cdot {[\\mathbf{a}^{(2)}]^{(k)}}$ to simplify the notation. Therefore, the total batch gradient calculation is given by:\n",
    "\n",
    "$$ \\mathbf{\\nabla}^{(2)} = \\sum_k \\nabla_k^{(2)} $$\n",
    "\n",
    "This first operation is relatively easy to vectorize because each column in $\\mathbf{A}^{(2)}$ corresponds to the $k^{th}$ instance of $_k\\mathbf{a}^{(2)}$. Therefore we can simply transpose $\\mathbf{A}^{(2)}$ and multiply it by the elementwise multiplications of $\\mathbf{y}^{(k)}$ and $_k\\mathbf{a}^{(3)}$:\n",
    "\n",
    "$$ \\mathbf{\\nabla}^{(2)} = -2(\\mathbf{Y}-\\mathbf{A}^{(3)})*\\mathbf{A}^{(3)}*(1-\\mathbf{A}^{(3)})\\cdot [\\mathbf{A}^{(2)}]^T $$\n",
    "\n",
    "For convenience, we define another variable, $\\mathbf{V}^{(2)}$ (called the sensitivity):\n",
    "\n",
    "$$ \\mathbf{V}^{(2)} = -2(\\mathbf{Y}-\\mathbf{A}^{(3)})*\\mathbf{A}^{(3)}*(1-\\mathbf{A}^{(3)}) $$\n",
    "\n",
    "such that:\n",
    "\n",
    "$$ \\mathbf{\\nabla}^{(2)} = \\mathbf{V}^{(2)}\\cdot [\\mathbf{A}^{(2)}]^T $$\n",
    "\n",
    "Further details about this calculation can be found in the lecture notes or the video here: https://youtu.be/WRr2e7mKCUY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "Similarly, the calculation of the gradient for the hidden layer, from our derivations was:\n",
    "\n",
    "$$  \\nabla_k^{(1)} = \\frac{\\partial J(\\mathbf{W})}{\\partial \\mathbf{z}^{(2)}}\\cdot \\mathbf{W}^{(2)} \\cdot \\text{diag}[{_k\\mathbf{a}^{(2)}}*(1-{_k\\mathbf{a}^{(2)}})] \\cdot \\mathbf{a}^{(1)}$$\n",
    "\n",
    "$$ \\mathbf{\\nabla}^{(1)} = \\sum_k \\nabla_k^{(1)} $$\n",
    "\n",
    "This can also be vectorized by recognizing that $\\frac{\\partial J(\\mathbf{W})}{\\partial \\mathbf{z}^{(2)}}$ is a matrix with columns equal to $\\frac{\\partial J(\\mathbf{W})}{\\partial {_k\\mathbf{z}^{(2)}}}$ for each instance $k$. We have already solved for this: $ \\mathbf{V}^{(2)} $. With some algebra, we can find that the computation is represented vectorized as:\n",
    "\n",
    "$$  \\mathbf{V}^{(1)} = \\mathbf{A}^{(2)} * (1-\\mathbf{A}^{(2)}) * [\\mathbf{W}^{(2)}]^T \\cdot \\mathbf{V}^{(2)} \\cdot \\mathbf{A}^{(1)}$$\n",
    "\n",
    "And we define an intermediate sensitivity variable $\\mathbf{V}^{(1)}$ as:\n",
    "\n",
    "$$ \\mathbf{V}^{(1)} =  \\mathbf{A}^{(2)} * (1-\\mathbf{A}^{(2)}) * [\\mathbf{W}^{(2)}]^T \\cdot \\mathbf{V}^{(2)}  $$\n",
    "\n",
    "such that: \n",
    "\n",
    "$$  \\mathbf{\\nabla}^{(1)} = \\mathbf{V}^{(1)}\\cdot [\\mathbf{A}^{(1)}]^{T} $$\n",
    "\n",
    "Now we can update the weight vectors according to:\n",
    "\n",
    "$$ \\mathbf{W}^{(l)} \\leftarrow \\mathbf{W}^{(l)} -\\eta \\mathbf{\\nabla}^{(l)} $$\n",
    "\n",
    "Let's get programming! All we really need to do is update the `_get_gradient` class method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(n_hidden=50, \n",
    "              C=0.1, # tradeoff L2 regularizer\n",
    "              epochs=400, # iterations\n",
    "              eta=0.001,  # learning rate\n",
    "              random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerPerceptronVectorized(TwoLayerPerceptron):\n",
    "    # just need a different gradient calculation\n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = -2*(Y_enc-A3)*A3*(1-A3)\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        gradW2 = V2 @ A2.T\n",
    "        gradW1 = V1 @ A1.T\n",
    "        \n",
    "        gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "        gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "        \n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        gradW1 += W1 * self.l2_C * 2\n",
    "        gradW2 += W2 * self.l2_C * 2 \n",
    "\n",
    "        return gradW1, gradW2, gradb1, gradb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 400/400"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n",
      "CPU times: user 1.97 s, sys: 55.6 ms, total: 2.02 s\n",
      "Wall time: 511 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nn = TwoLayerPerceptronVectorized(**params)\n",
    "\n",
    "nn.fit(X_train, y_train, print_progress=50)\n",
    "yhat = nn.predict(X_test)\n",
    "print('Accuracy:',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(nn.cost_)), nn.cost_)\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Back to slides] \n",
    "\n",
    "## Adding complexity and functionality\n",
    "So now we can calculate the gradient quickly, but we need to account for more optimization methods in this new space because the optmiization surface can be highly non-convex. We need to steer clear of local optima as best as possible.\n",
    "\n",
    "To start, let's add: \n",
    "- some momentum to the calculation \n",
    "- $\\eta$ learning rate decay\n",
    "- perform mini-batching of the gradient updates \n",
    "- random reshuffling of the inputs\n",
    "\n",
    "For cooling the learning rate, at epoch $k$:\n",
    "$$ \\mathbf{W}_{k+1} = \\mathbf{W}_k - \\frac{\\eta}{1+\\epsilon\\cdot k} \\cdot \\rho_{k} $$\n",
    "\n",
    "For momentum:\n",
    "$$ \\rho_{k} = \\nabla J(\\mathbf{W}_k) + \\alpha\\rho_{k-1} $$\n",
    "\n",
    "For decaying the learning rate:\n",
    "$$ \\eta_k = \\max(\\eta_0^{(1+k\\cdot d)},\\frac{\\eta_0}{1000}) $$\n",
    "\n",
    "where $k$ is the epoch, $\\eta_0$ is the starting value of $\\eta$, $\\eta<1$, and $d<1$, the decreasing constant,. \n",
    "\n",
    "Now let's build in these vales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "# Original Author: Sebastian Raschka\n",
    "# This is the optional book we use in the course, excellent intuitions and straightforward programming examples\n",
    "# please note, however, that this code has been manipulated to reflect our assumptions and notation.\n",
    "class TLPVectorizedMiniBatch(TwoLayerPerceptronVectorized):\n",
    "    def __init__(self, alpha=0.1, decrease_const=0.0, shuffle=True, minibatches=1, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "        \n",
    "    def fit(self, X, y, print_progress=0):\n",
    "        \"\"\" Learn weights from training data.\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.b1, self.b2 = self._initialize_weights()\n",
    "        \n",
    "        # start momentum at zero for previous updates\n",
    "        rho_W1_prev = np.zeros(self.W1.shape) # for momentum\n",
    "        rho_W2_prev = np.zeros(self.W2.shape) # for momentum\n",
    "        rho_b1_prev = np.zeros(self.b1.shape) # for momentum\n",
    "        rho_b2_prev = np.zeros(self.b2.shape) # for momentum\n",
    "\n",
    "        self.cost_ = []\n",
    "        for k in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            eta = self.eta**(1+self.decrease_const*k)# decreasing learning rate\n",
    "            eta = max(eta,self.eta/1000)\n",
    "            # there are many forms of adaptive learning rates out there!\n",
    "\n",
    "            if print_progress>0 and (k+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (k+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                ridx = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc = X_data[ridx], Y_enc[:, ridx]\n",
    "\n",
    "            # use numpy split to split into equal sized batches \n",
    "            # num batches== self.minibatches\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3 = self._feedforward(X_data[idx],\n",
    "                                                       self.W1,\n",
    "                                                       self.W2,\n",
    "                                                       self.b1,\n",
    "                                                       self.b2\n",
    "                                                      )\n",
    "                \n",
    "                cost = self._cost(A3,Y_enc[:, idx],self.W1,self.W2)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                gradW1, gradW2, gradb1, gradb2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2,\n",
    "                                                  Y_enc=Y_enc[:, idx],\n",
    "                                                  W1=self.W1, W2=self.W2)\n",
    "\n",
    "                rho_W1, rho_W2 = eta * gradW1, eta * gradW2\n",
    "                rho_b1, rho_b2 = eta * gradb1, eta * gradb2\n",
    "                self.W1 -= (rho_W1 + (self.alpha * rho_W1_prev)) # update with momentum\n",
    "                self.W2 -= (rho_W2 + (self.alpha * rho_W2_prev)) # update with momentum\n",
    "                self.b1 -= (rho_b1 + (self.alpha * rho_b1_prev))\n",
    "                self.b2 -= (rho_b2 + (self.alpha * rho_b2_prev))\n",
    "                rho_W1_prev, rho_W2_prev = rho_W1, rho_W2\n",
    "                rho_b1_prev, rho_b2_prev = rho_b1, rho_b2\n",
    "                \n",
    "\n",
    "            self.cost_.append(mini_cost)\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "params['epochs'] = 100\n",
    "params['eta'] = 0.1\n",
    "nn_mini = TLPVectorizedMiniBatch(**params,\n",
    "                          alpha=0.1,# momentum calculation\n",
    "                          decrease_const=0.1, # decreasing eta\n",
    "                          minibatches=len(X_train)/32, # minibatch size\n",
    "                          shuffle=True)\n",
    "\n",
    "    \n",
    "nn_mini.fit(X_train, y_train, print_progress=50)\n",
    "yhat = nn.predict(X_test)\n",
    "print('Accuracy:',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the mean of each minibatch epoch\n",
    "cost_avgs = [np.mean(x) for x in nn_mini.cost_]\n",
    "\n",
    "plt.plot(range(len(cost_avgs)), cost_avgs, color='red')\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using different optimization techniques\n",
    "While the above implementation is good for a number of applications, we have yet to use more advanced optimization algorithms. Why is that? Shouldn't we remedy this situation?\n",
    "\n",
    "### Self-test: Should we try quasi-Newton methods on the MLP?\n",
    "- A. Yes. Quasi-Newton methods converge much faster\n",
    "- B. Yes. Quasi-Newton methods are guaranteed to find the global optimum\n",
    "- C. No. There is no guarantee that Quasi-Newton methods will work better in a non-convex space.\n",
    "- D. No. With so many weights, the Hessian calculation is too complex. \n",
    "\n",
    "___\n",
    "\n",
    "## Quasi-Newton updates for the MLP\n",
    "Let's try to perform L-BFGS with on the previous models. The easy way of performing this is to add wrapper methods to the `_get_gradient` function in order to pack and unpack the data into the Weights matrices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_bfgs\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "class TLPVectorizedBFGS(TwoLayerPerceptronVectorized):\n",
    "    \n",
    "    def __init__(self, gtol=1e-5, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.gtol = gtol\n",
    "        \n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _pack(in1, in2, in3, in4):\n",
    "        '''Pack and flatten input vectors '''\n",
    "        return np.hstack((in1.flatten(),in2.flatten(),in3.flatten(),in4.flatten()))\n",
    "    \n",
    "    def _unpack(self, in_tot):\n",
    "        '''Undo packing according to layer weight sizes'''\n",
    "        start = 0\n",
    "        end = self.W1.size\n",
    "        out1 = in_tot[start:end].reshape(self.W1.shape)\n",
    "        start = end\n",
    "        end += self.W2.size\n",
    "        out2 = in_tot[start:end].reshape(self.W2.shape)\n",
    "        start = end\n",
    "        end += self.b1.size\n",
    "        out3 = in_tot[start:end].reshape(self.b1.shape)\n",
    "        start = end\n",
    "        end += self.b2.size\n",
    "        out4 = in_tot[start:end].reshape(self.b2.shape)\n",
    "        return out1, out2, out3, out4\n",
    "    \n",
    "    def _calc_cost_gradient_packed(self,W,X_data,Y_enc):\n",
    "        '''Unpack and get cost, gradient for bfgs'''\n",
    "        W1, W2, b1, b2 = self._unpack(W) \n",
    "        # feedforward all instances\n",
    "        A1, Z1, A2, Z2, A3 = self._feedforward(X_data, W1, W2, b1, b2)\n",
    "        \n",
    "        cost = np.sum((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        cost = cost + L2_term\n",
    "        #perform back prop to get gradients\n",
    "        gradW1,gradW2,gradb1,gradb2 = self._get_gradient(A1=A1, A2=A2, A3=A3,Z1=Z1, Z2=Z2, Y_enc=Y_enc,\n",
    "                                         W1=W1, W2=W2)\n",
    "        return cost, self._pack(gradW1,gradW2,gradb1,gradb2)\n",
    "    \n",
    "    def _cost_packed(self,W,X_data,Y_enc):\n",
    "        '''Unpack and calculate MSE for bfgs'''\n",
    "        W1, W2, b1, b2 = self._unpack(W)\n",
    "        _, _, _, _, A3 = self._feedforward(X_data,W1,W2,b1,b2)\n",
    "        return np.sum((Y_enc-A3)**2)\n",
    "    \n",
    "    def fit(self,X,y,print_progress=0):\n",
    "        '''Learn weights from training data'''\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.b1, self.b2 = self._initialize_weights()\n",
    "        \n",
    "        # make initial matrices into single row vector\n",
    "        W = self._pack(self.W1,self.W2, self.b1, self.b2)\n",
    "        \n",
    "        if print_progress>0:\n",
    "            def callback(xd):\n",
    "                callback.counter += 1\n",
    "                if callback.counter%print_progress==0:\n",
    "                    sys.stderr.write('\\rEpoch: %d/%d (max)' % (callback.counter,callback.epochs))\n",
    "                    sys.stderr.flush()\n",
    "\n",
    "            callback.counter = 0\n",
    "            callback.epochs = self.epochs\n",
    "            \n",
    "        else:\n",
    "            callback = None\n",
    "            \n",
    "        # compute gradient optimum with bfgs\n",
    "        W_best,_,props = fmin_l_bfgs_b(\n",
    "                        x0=W,\n",
    "                        func=self._calc_cost_gradient_packed,\n",
    "                        maxfun=self.epochs,\n",
    "                        callback=callback,\n",
    "                        pgtol=self.gtol,\n",
    "                        args=(X_data, Y_enc))\n",
    "        \n",
    "        self.W1, self.W2, self.b1, self.b2 = self._unpack(W_best)\n",
    "        if print_progress:\n",
    "            print(props)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nn_bfgs = TLPVectorizedBFGS(**params, gtol=1e-3)\n",
    "\n",
    "nn_bfgs.fit(X_train, y_train, print_progress=1)\n",
    "yhat = nn_bfgs.predict(X_test)\n",
    "print('Accuracy:',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! That was pretty fast compared to the other implementations we have seen. But don't let this fool you. The L-BFGS implementation is great for smaller datasets, but minibatch/stochastic methods are more optimal as the number of instances in our data gets larger. Eventually they over power the L-BFGS algorithm's ability to move along the curvature effectively and efficiently.  \n",
    "___\n",
    "# Using a bigger, more diverse dataset\n",
    "\n",
    "Now let's load in a more diverse, harder to classify dataset: Fashion MNIST\n",
    "https://www.kaggle.com/zalando-research/fashionmnist\n",
    "\n",
    "**Labels**\n",
    "\n",
    "Each training and test example is assigned to one of the following labels:\n",
    "\n",
    "- T-shirt/top\n",
    "- Trouser\n",
    "- Pullover\n",
    "- Dress\n",
    "- Coat\n",
    "- Sandal\n",
    "- Shirt\n",
    "- Sneaker\n",
    "- Bag\n",
    "- Ankle boot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more data \n",
    "# Let's use Raschka's implementation for using the fashion mnist dataset:\n",
    "# https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    " \n",
    "def load_mnist(path, kind='fashion_train'):\n",
    "    \"\"\"Load Fashion MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte' % kind)\n",
    "        \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    " \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_mnist('data/', kind='fashion_train')\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))\n",
    "\n",
    "X_test, y_test = load_mnist('data/', kind='fashion_t10k')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))\n",
    "\n",
    "# don't forget to normalize\n",
    "X_train = X_train/255.0 - 0.5\n",
    "X_test = X_test/255.0 - 0.5\n",
    "\n",
    "print(np.min(X_train),np.max(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True,)\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = X_train[y_train == i][0].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0,T-shirt/top\n",
    "# 1,Trouser\n",
    "# 2,Pullover\n",
    "# 3 Dress\n",
    "# 4 Coat\n",
    "# 5 Sandal\n",
    "# 6 Shirt\n",
    "# 7 Sneaker\n",
    "# 8 Bag\n",
    "# 9 Ankle boot\n",
    "\n",
    "fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True,)\n",
    "ax = ax.flatten()\n",
    "imgs_of_one_class = X_train[y_train == 2]\n",
    "for i in range(25):\n",
    "    img = imgs_of_one_class[i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L-BFGS Optimization for Fashion MNIST\n",
    "This is a nice test to show that mini-batching has better conversion properties for some datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nn_bfgs = TLPVectorizedBFGS( \n",
    "                      n_hidden=50, \n",
    "                      C=0.1, \n",
    "                      epochs=100, # max iterations\n",
    "                      gtol=1e-9,\n",
    "                      random_state=1)\n",
    "\n",
    "nn_bfgs.fit(X_train, y_train, print_progress=1)\n",
    "yhat = nn_bfgs.predict(X_test)\n",
    "print('Validation acc:',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniBatch Gradient Descent for Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nn_mini = TLPVectorizedMiniBatch( \n",
    "                          n_hidden=50, \n",
    "                          C=0.1,  \n",
    "                          epochs=40, \n",
    "                          eta=0.1,\n",
    "                          alpha=0.05,\n",
    "                          decrease_const=0.1,\n",
    "                          minibatches=len(X_train)/256, \n",
    "                          shuffle=True,\n",
    "                          random_state=1)\n",
    "\n",
    "nn_mini.fit(X_train, y_train, print_progress=1)\n",
    "yhat = nn_mini.predict(X_test)\n",
    "print('Validation acc:',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the mean of each minibatch epoch\n",
    "cost_avgs = [np.mean(x) for x in nn_mini.cost_]\n",
    "\n",
    "plt.plot(range(len(cost_avgs)), cost_avgs, color='red')\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names=[\n",
    "    'T-shirt/top',\n",
    "    'Trouser',\n",
    "    'Pullover',\n",
    "    'Dress',\n",
    "    'Coat',\n",
    "    'Sandal',\n",
    "    'Shirt',\n",
    "    'Sneaker',\n",
    "    'Bag',\n",
    "    'Ankle boot']\n",
    "\n",
    "# get first 25 confusions\n",
    "miscl_img = X_test[y_test != yhat][:25]\n",
    "correct_lab = y_test[y_test != yhat][:25]\n",
    "miscl_lab = yhat[y_test != yhat][:25]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True, figsize=(10,10))\n",
    "ax = ax.flatten()\n",
    "for i in range(25):\n",
    "    img = miscl_img[i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "    ax[i].set_title('%s, (p:%s)' % (label_names[correct_lab[i]],label_names[miscl_lab[i]]))\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While these misclassifications are looking mostly okay, we can do better. Is 85% good for Fashion MNIST?  We can check out the leaderboard here: https://paperswithcode.com/sota/image-classification-on-fashion-mnist\n",
    "\n",
    "But we have some additional implementation details that are needed for practical implementation of neural networks, in order to get them training more efficiently and more meaningfully. These are visited in the next notebook, `08. Practical Implementation of Neural Networks`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook you learned:\n",
    "- an introduction to neural networks notation and programming\n",
    "- using a two-layer perceptron architecture with sigmoid activations\n",
    "- optimizing this network with mini-batch descent and Quasi-newton methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
